# プレゼンテーション原稿

では、これから視聴者参加型ライブ配信システム、改めAster Connectの制作成果発表をいたします。よろしくお願いいたします。

本日最後の発表ということで皆さん疲れていらっしゃることと思いますけれども、エンタメ系のシステムということで他の方々とは違った面白さがあるので、ぜひ聞いていただけたらと存じます。

## 目次

全体的にはこちらのような流れでお話ししてまいります。

## 概要

最初に、本実習およびシステムの概要についてご説明いたします。

### 導入

(では、) 突然ですが、皆様は「ユージェネ」というサービスをご存知でしょうか。ゲーム界隈では少し話題になったため、もしかしたらご存じの方もいらっしゃるかもしれません。

<クリック>

ユージェネとは、LIVEとGAMEの融合による「新時代エンターテインメント」をうたったサービスでございます。  
サービスの内容も斬新で挑戦的ではありましたが、XRの活用や高いリアルタイム性の実現など技術面においても非常に意欲的であり、私はその技術的野心に大いに刺激されました。  
その影響で今回「視聴者参加型ライブ配信システム」というテーマを選定することといたしました。

### 概要

というわけで本システムの概要についてお話しいたします。

本システムは、仮想空間のステージ上でキャラクターがライブ配信を行うためのシステムでございます。3D空間を活用することで従来のライブ配信よりも高い臨場感を味わえます。また、エールやミニゲームといった独自の仕組みで、インタラクティブ、双方向的なコミュニケーションを提供します。

### アプリ構成

次に機能についてお話ししたいのですが、本システムは構成が複雑であるため、まずは全体的な部分からお話ししてまいります。

本システムは複数のアプリケーションで構成されております。配信を見る人や配信を提供する人など、様々な人が関わりあうためです。

では、各要素について説明してまいります。

左側のクライアントは、ライブを視聴するためのアプリケーションで、視聴者であるユーザが使用します。現時点ではWindowsとAndroidに対応しています。

中央のサーバはシステムの中心となるアプリケーションです。各アプリケーションの通信を中継したりします。

右上のコントローラは、キャラクターの位置や音声を送信するためのアプリケーションで、キャラクターを演じるプレゼンタが使用します。ステージ上のオブジェクトを持ち上げるために、VRのヘッドセットを使用します。

その下のコンソールは、ライブを制御するためにサーバに指示を出すアプリケーションで、サポータという人が使用します。プレゼンタがサポータを兼任することも一応可能です。

## 機能

というわけで、機能の説明に入ってまいります。

<機能一覧みたいなのがあったほうがいい?→セクション内目次とかあったらいいかも>

まずはクライアント、視聴者側から見た機能をご紹介します。

### コメント

こちらの機能では、一般的なライブ配信サービスと同様にコメントを送れます。送られたコメントはこのように積み重なって表示されます。なお、コメントには20文字の制限を設けております。

### エール

続いて、こちらの機能ではエールというアイテムを贈れます。贈ったエールはステージ上に出現し、一定時間後にステージからなくなります。このエールという仕組みによって、ライブの話題や視聴者ごとの個性が生まれます。

### 視点操作

続いて、こちらの機能ではステージを好きな角度や位置から見られます。動画配信のような従来の形態では実現できない機能でございます。

### ミニゲーム

続いて、こちらの機能ではキャラクターとのミニゲームを楽しめます。今回はじゃんけんのミニゲームを実装いたしました。好成績をおさめたユーザは結果発表後に名前が表示されます。

次にコントローラ、配信する側から見た機能をご紹介します。

### フルボディトラッキング

こちらの機能では、プレゼンタの全身の姿勢と位置を読み取ってステージにいるキャラクターに反映できます。狭い場所などで全身を自由に動かせない場合は、読み取る範囲を上半身のみなどに限定できます。

### エールつかみ

続いて、こちらの機能ではVRのコントローラのグリップを握ることでステージ上のエールをつかめます。このとき、そのエールを贈ったユーザの名前も同時に表示されます。また、つかんだエールは放り投げたり別の手に持ち替えたりできます。

### スティックでの移動

続いて、こちらの機能ではVRのコントローラのアナログスティックを倒すことでステージ内を移動できます。これにより、実際に歩いて移動できないような場合でもエールをつかみに行ったりできるようになります。

### キャラクター変更

続いて、こちらの機能ではステージに表示するキャラクターを変えられます。本システムではプレゼンタが複数いるライブも開催できるため、その際には必須の機能でございます。  
なお、キャラクターの3DモデルをVRMという形式で統一することで、決まった手順でキャラクターを追加できるようにいたしました。

最後にコンソール、同じく配信する側から見た機能をご紹介します。

### サーバ状態遷移

こちらの機能では、ライブの開始や終了ができます。サーバには図のような3つの状態があるのですが、こちらの機能を用いてそれぞれ切り替えます。

### ミニゲーム進行

続いて、こちらの機能ではライブのミニゲームを進行できます。操作ミスを防止するために不要なボタンは押せないようにいたしました。

## 画面

では、これから各アプリケーションの画面についてご説明いたします。

### クライアント

クライアントの画面遷移はこのようになっております。

タイトル画面の [参加する] ボタンを押すと、その時点でのサーバの状態によって異なる画面に遷移いたします。

中央の待機画面ではコメント機能が利用できるので、キャラクターへの応援やユーザ同士の交流に役立ちます。ライブが始まると自動的にライブ画面に遷移します。

### ライブ画面

次にエールの贈り方とライブ画面についてご説明いたします。

まず、画面左下の入力欄からコメントを送信できます。送信したコメントはその上のコメント欄に表示されます。

次に、画面左上のボタンでライブから抜けてタイトル画面に戻れます。その際、確認のダイアログが出るようにいたしました。

そして、画面右下のボタンを押すとエール一覧が開きます。

### エール一覧

エール一覧はこのようになっております。ここから適当なエールを選ぶとエール確認画面が表示されます。

### エール確認画面

エール確認画面では、エールの詳細が確認できます。  
エールにはSとMの2種類のサイズがあり、より大きいMサイズのほうが早く消えるようになっております。確認が済んだら、緑色の [贈る] ボタンでエールを贈ります。なお、中央には価格の情報が表示されますが、こちらはゲーム内通貨の機能を実装したときに使用する予定でした。今回はその機能の開発までは至らなかったため、この項目には特に意味はございません。

### コントローラ

続いて、コントローラの画面遷移はこのようになっております。

こちらもクライアント同様、タイトル画面の [接続する] ボタンを押すとサーバの状態によって異なる画面に遷移いたします。  
サーバが準備中の場合でもライブ画面に遷移する点がクライアントと異なります。

### ライブ画面

コントローラのライブ画面はこのようになっております。

視界の正面に透明なパネルを設け、その上にUI要素を表示する形となっております。左上のボタンと左下のコメントについてはクライアントと同様です。また、右上にはライブに参加中のユーザの数が表示されます。

### コンソール

そして、コンソールの画面遷移はこのようになっております。

メイン画面では、[ライブ] メニューからライブの状態を遷移させ、中央部のボタンでミニゲームを進行させます。また、下部のステータスバーには命令の実行結果が表示されます。なお、中央左側はタブになっており、新しいミニゲームを追加しやすいようにしております。

### 配色

最後に、画面の配色についてお話しいたします。このスライドでも同様ですが、メインとなる色にはJIS安全色を使用しております。これによって、より多くの人にとって使いやすいアプリケーションを目指しました。

## 技術

では、ここからは本システムの技術的な側面をご説明いたします。

### 技術構成

<段落変えでクリック>

ここでは技術構成についてお話しします。本実習では外部のソフトウェアやサービスの利用経験を積むという目的を掲げているため、そちらに引っ掛けたお話となります。  
全体的な構成はスライドにある通りでございます。クライアント、サーバ、コントローラはUnityで、コンソールはWPFで開発いたしました。両者とも自分が書き慣れたC#で実装できるため、この二つを選定いたしました。  
では、図中の各ソフトウェア・サービスについてお話しします。

まず、姿勢をキャラクターに反映するためにPose AIというソフトウェアを利用いたしました。類似のソフトは多く見つかりましたが、いずれも高性能なPCが必要だったりリアルタイムでのトラッキングができなかったりしたため、こちらを利用することとなりました。  
しかしこのPose AI、イギリスのきちんとした企業が開発していることに反してサードパーティの情報が非常に少ないです。具体的に言うと、自分が調べた範囲だと日本語の紹介記事が1件見つかっただけでした。そのため、嫌でも公式ドキュメントに頼らざるを得ない状況を体験できました。

続いて、キャラクターの口を動かすためにOculus Lipsyncというプラグインを利用いたしました。機能上はキャラクターの口が動くのですが、開発期間中にVRMのメジャーアップデートがあり、新しいバージョンに対応した結果、古いバージョンのモデルが動かなくなってしまいました。ソフトウェア開発においては、こうした外部の変化にどう対処していくかを考えることも大切だと思いました。

続いて、VRアプリを開発するためにXR Interaction Toolkitというパッケージを利用いたしました。これにより、Meta Quest 2以外にも様々なVRデバイスで動作するようになっております。

アプリケーション間の通信においてはNetcode for GameObjectというライブラリを利用いたしました。こちらでは、ステージ上のオブジェクトの同期やコードの遠隔実行などの全般的な通信処理を行っております。今回はインターネットを経由した通信も対象としたため、ルータのDMZ機能やファイアウォールの設定を実際に行うことができました。

また、音声の配信はUnityの標準機能では困難だったため、Vivoxというサービスを利用いたしました。利用にあたっては、Vivoxのアカウントを取得してUnityのプロジェクトと紐づけるなど、APIの使い方を学ぶ以外の作業がございました。

## 実演

では、これから実演に移ってまいります。実演では、私がプレゼンターとなって、実際にインターネット上でライブ配信をしてまいります。

こちらの画面の左側にユーザが見るクライアントの画面を、右上にプレゼンタが見るコントローラの画面を、右下にサポータが操作するコンソールの画面を表示いたします。なお、クライアントとコントローラの画面はこちらの実機の画面をミラーリングしたものとなっております。

ちなみに、私はプレゼンタを務める都合でその他の操作ができないため、その部分はこちらの友人にお願いいたしました。

では、これから準備をいたしますので少々お待ちください。